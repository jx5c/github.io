<!DOCTYPE html>
<html>
<head><meta charset="utf-8">
	<title>Jian Xiang</title>
	<link href="../homepage.css" rel="stylesheet" type="text/css" />
	<link href="img/smile.icon.png" rel="shortcut icon" />
</head>
<body>
<div class="container">

    <div class="navbar">
      <hr style="height: 2px; width: 100%;">
      <table style="text-align: left; margin-left: auto; margin-right: auto;" border="0" cellpadding="0" cellspacing="0">
	<tbody><tr align="center">
    	        <td width="10%">
		  <a href="../index.html">Home</a> 
		</td>
    	        <td width="10%">
		  <a href="../publications.html">Publications</a> 
		</td>
    	        <td width="10%">
		  <a href="../teaching.html">Teaching</a> 
		</td>
    	        <td width="10%">
		  <a href="../resources.html">Resources</a> 
		</td>
    	        </td>
    	        <td width="10%">
		  <a href="../contact.html">Contact</a> 
		</td>
</tr>
</tbody>
</table>
  <hr style="height: 2px; width: 100%;">
</div>


<div class="content">
  <div class="contentinner">
  <div class="abstract">
  <div class="pubtitle">Measuring Robustness in Cyber-Physical Systems under Sensor Attacks</div>
  <div class="pubauthors">Jian Xiang, Ruggero Lanotte, Simone Tini, Stephen Chong, and Massimo Merro. </div>
  <div class="booktitle"><span class="booktitle">Journal: Nonlinear Analysis: Hybrid Systems
</span> (To appear) </div>
  <div class="publinks">
  [&nbsp;
  <a href="../abstracts/measure2024.html" class="publink" >Abstract</a>&nbsp;|&nbsp;
  <a href="https://arxiv.org/pdf/2403.05829" class="publink";">PDF</a>&nbsp;|&nbsp;
  <a href="../bib/measure2024.bib" class="publink">BibTeX</a>&nbsp;
  ]
</div>
<div class="abstractheader">Abstract.</div>
  <div class="abstracttext">
  <p class="firstpara">This paper contributes a formal framework for quantitative analysis of bounded sensor attacks on cyber-physical systems, using the formalism of differential dynamic logic. Given a precondition and postcondition of a system, we formalize two quantitative safety notions, quantitative forward and backward safety, which respectively express (1) how strong the strongest postcondition of the system is with respect to the specified postcondition, and (2) how strong the specified precondition is with respect to the weakest precondition of the system needed to ensure the specified postcondition holds. We introduce two notions, forward and backward robustness, to characterize the robustness of a system against sensor attacks as the loss of safety. To reason about robustness, we introduce two simulation distances, forward and backward simulation distances, which are defined based on the behavioral distances between the original system and the system with compromised sensors. Forward and backward distances, respectively, characterize upper bounds of the degree of forward and backward safety loss caused by the sensor attacks. We verify the two simulation distances by expressing them as modalities, i.e., formulas of differential dynamic logic, and develop an ad-hoc proof system to reason with such formulas. We showcase our formal notions and reasoning techniques on two non-trivial case studies: an autonomous vehicle that needs to avoid collision and a water tank system.
</p>
</div>
</p>
</div>
  
</div>
</div>


  
</div>
</body>
</html>
